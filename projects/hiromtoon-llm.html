<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hiromtoon LLM - Hiromtoon's Portfolio</title>
    <link rel="stylesheet" href="../css/style.css">
    <style>
        .project-detail {
            padding: 4rem 0;
            max-width: 900px;
            margin: 0 auto;
        }
        .back-link {
            color: var(--accent-primary);
            text-decoration: none;
            display: inline-block;
            margin-bottom: 2rem;
        }
        .back-link:hover {
            color: var(--accent-secondary);
        }
        .project-header {
            margin-bottom: 3rem;
        }
        .project-header h1 {
            font-size: 2.5rem;
            margin-bottom: 1rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .project-meta {
            display: flex;
            gap: 2rem;
            margin-bottom: 2rem;
        }
        .meta-item {
            color: var(--text-secondary);
        }
        .meta-label {
            font-weight: bold;
            color: var(--accent-primary);
        }
        .section {
            margin-bottom: 3rem;
        }
        .section h2 {
            font-size: 1.75rem;
            margin-bottom: 1.5rem;
            color: var(--text-primary);
        }
        .section h3 {
            font-size: 1.3rem;
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: var(--accent-primary);
        }
        .section p, .section li {
            color: var(--text-secondary);
            line-height: 1.8;
            margin-bottom: 1rem;
        }
        .section ul {
            margin-left: 1.5rem;
        }
        .code-block {
            background-color: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: 0.5rem;
            padding: 1.5rem;
            margin: 1rem 0;
            overflow-x: auto;
        }
        .code-block code {
            font-family: 'Courier New', monospace;
            color: var(--text-primary);
            font-size: 0.9rem;
            line-height: 1.6;
        }
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 1rem;
            margin: 2rem 0;
        }
        .stat-box {
            background-color: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: 0.5rem;
            padding: 1.5rem;
            text-align: center;
        }
        .stat-value {
            font-size: 2rem;
            font-weight: bold;
            color: var(--accent-secondary);
            display: block;
        }
        .stat-label {
            color: var(--text-secondary);
            font-size: 0.875rem;
            margin-top: 0.5rem;
        }
        .highlight-box {
            background-color: var(--bg-card);
            border-left: 4px solid var(--accent-secondary);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0.5rem;
        }
        .pipeline {
            background-color: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: 0.5rem;
            padding: 2rem;
            margin: 2rem 0;
        }
        .pipeline-step {
            display: flex;
            align-items: center;
            margin: 1rem 0;
            padding: 1rem;
            background-color: var(--bg-secondary);
            border-radius: 0.5rem;
        }
        .step-number {
            background: var(--accent-gradient);
            color: white;
            width: 2rem;
            height: 2rem;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            margin-right: 1rem;
            flex-shrink: 0;
        }
        .step-content h4 {
            color: var(--text-primary);
            margin-bottom: 0.5rem;
        }
        .step-content p {
            color: var(--text-secondary);
            margin: 0;
        }
    </style>
</head>
<body>
    <!-- Header -->
    <header>
        <nav class="container">
            <div class="logo">Hiromtoon</div>
            <ul class="nav-links">
                <li><a href="../index.html">Home</a></li>
                <li><a href="../index.html#projects">Projects</a></li>
                <li><a href="../index.html#contact">Contact</a></li>
            </ul>
        </nav>
    </header>

    <!-- Project Detail -->
    <main class="project-detail container">
        <a href="../index.html#projects" class="back-link">â† Back to Projects</a>

        <div class="project-header">
            <h1>ğŸ¤– Hiromtoon LLM</h1>
            <p class="hero-subtitle">è‡ªåˆ†ã®Twitterãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ãŸã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«</p>

            <div class="project-meta">
                <div class="meta-item">
                    <span class="meta-label">æœŸé–“:</span> 2025å¹´9æœˆã€œç¾åœ¨
                </div>
                <div class="meta-item">
                    <span class="meta-label">å½¹å‰²:</span> MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
                </div>
                <div class="meta-item">
                    <span class="meta-label">æ‰‹æ³•:</span> QLoRA Fine-tuning
                </div>
            </div>

            <div class="tech-tags">
                <span class="tag">PyTorch</span>
                <span class="tag">QLoRA</span>
                <span class="tag">Gemma3-27B</span>
                <span class="tag">HuggingFace</span>
                <span class="tag">PEFT</span>
                <span class="tag">CUDA</span>
                <span class="tag">bitsandbytes</span>
            </div>
        </div>

        <!-- Stats -->
        <div class="stats">
            <div class="stat-box">
                <span class="stat-value">19,320</span>
                <span class="stat-label">å­¦ç¿’å…ƒãƒ„ã‚¤ãƒ¼ãƒˆæ•°</span>
            </div>
            <div class="stat-box">
                <span class="stat-value">27B</span>
                <span class="stat-label">ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</span>
            </div>
            <div class="stat-box">
                <span class="stat-value">32GB</span>
                <span class="stat-label">GPU VRAM</span>
            </div>
            <div class="stat-box">
                <span class="stat-value">~2æ—¥</span>
                <span class="stat-label">å­¦ç¿’æ™‚é–“</span>
            </div>
        </div>

        <!-- Overview -->
        <section class="section">
            <h2>ğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦</h2>
            <p>
                è‡ªåˆ†è‡ªèº«ã®Twitterã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ãƒ¼ã‚¿19,320ä»¶ã‚’ä½¿ç”¨ã—ã¦ã€Google ã® Gemma3-27B ãƒ¢ãƒ‡ãƒ«ã‚’QLoRAï¼ˆQuantized Low-Rank Adaptationï¼‰ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚è‡ªåˆ†ã®æ–‡ä½“ã€æ€è€ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã€èˆˆå‘³é–¢å¿ƒã‚’å†ç¾ã™ã‚‹ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚
            </p>
            <div class="highlight-box">
                <p><strong>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®çš„:</strong> å€‹äººã®çŸ¥è­˜ãƒ»æ€è€ƒãƒ»æ–‡ä½“ã‚’å­¦ç¿’ã—ãŸAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚’ä½œæˆã—ã€å°†æ¥çš„ã«OpenWebUIã‚’é€šã˜ã¦æ—¥å¸¸çš„ã«æ´»ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã€‚</p>
            </div>
        </section>

        <!-- Why QLoRA -->
        <section class="section">
            <h2>ğŸ¯ ãªãœQLoRAã‚’é¸ã‚“ã ã®ã‹</h2>
            <p>
                27Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ã€è‡ªå®…GPUç’°å¢ƒï¼ˆRTX 5060 Ti Ã— 2ã€åˆè¨ˆ32GB VRAMï¼‰ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã€QLoRAæ‰‹æ³•ã‚’æ¡ç”¨ã—ã¾ã—ãŸã€‚
            </p>

            <h3>QLoRAã®ç‰¹å¾´</h3>
            <ul>
                <li><strong>4-bité‡å­åŒ–:</strong> ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’75%å‰Šæ¸›ã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å¤§å¹…ã«å‰Šæ¸›</li>
                <li><strong>LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼:</strong> å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã›ãšã€ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®ã¿ã‚’å­¦ç¿’ï¼ˆé«˜åŠ¹ç‡ï¼‰</li>
                <li><strong>æ€§èƒ½ç¶­æŒ:</strong> Full Fine-tuning ã¨åŒç­‰ã®ç²¾åº¦ã‚’ã€1/4ã®ãƒ¡ãƒ¢ãƒªã§å®Ÿç¾</li>
                <li><strong>ã‚³ã‚¹ãƒˆå‰Šæ¸›:</strong> ã‚¯ãƒ©ã‚¦ãƒ‰GPUä¸è¦ã€è‡ªå®…ç’°å¢ƒã§å­¦ç¿’å¯èƒ½</li>
            </ul>

            <div class="highlight-box">
                <p>
                    <strong>ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®æˆæœ:</strong><br>
                    é€šå¸¸ã®Full Fine-tuning: ~100GB VRAM å¿…è¦<br>
                    QLoRA Fine-tuning: 18GB VRAM ã§å®Ÿç¾ï¼ˆ82%å‰Šæ¸›ï¼‰
                </p>
            </div>
        </section>

        <!-- Training Pipeline -->
        <section class="section">
            <h2>ğŸ”„ å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h2>
            <div class="pipeline">
                <div class="pipeline-step">
                    <div class="step-number">1</div>
                    <div class="step-content">
                        <h4>ãƒ‡ãƒ¼ã‚¿åé›†</h4>
                        <p>Twitterã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰19,320ä»¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’æŠ½å‡º</p>
                    </div>
                </div>

                <div class="pipeline-step">
                    <div class="step-number">2</div>
                    <div class="step-content">
                        <h4>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†</h4>
                        <p>ãƒªãƒ—ãƒ©ã‚¤ã€ãƒªãƒ„ã‚¤ãƒ¼ãƒˆã‚’é™¤å¤– â†’ 16,005ä»¶ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ„ã‚¤ãƒ¼ãƒˆã«çµã‚Šè¾¼ã¿</p>
                    </div>
                </div>

                <div class="pipeline-step">
                    <div class="step-number">3</div>
                    <div class="step-content">
                        <h4>ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›</h4>
                        <p>Gemma3 Instruction format ã«å¤‰æ›ï¼ˆuser/model ã‚¿ãƒ¼ãƒ³å½¢å¼ï¼‰</p>
                    </div>
                </div>

                <div class="pipeline-step">
                    <div class="step-number">4</div>
                    <div class="step-content">
                        <h4>ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</h4>
                        <p>Train: 80% (12,804ä»¶) / Validation: 20% (3,201ä»¶)</p>
                    </div>
                </div>

                <div class="pipeline-step">
                    <div class="step-number">5</div>
                    <div class="step-content">
                        <h4>QLoRAå­¦ç¿’</h4>
                        <p>Gemma3-27B ã‚’ 4-bité‡å­åŒ– + LoRA ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã§å­¦ç¿’ï¼ˆ3 Epochï¼‰</p>
                    </div>
                </div>

                <div class="pipeline-step">
                    <div class="step-number">6</div>
                    <div class="step-content">
                        <h4>ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤</h4>
                        <p>Base model + LoRA adapter ã‚’çµ±åˆã—ã€Ollama å½¢å¼ã«å¤‰æ›</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Technical Implementation -->
        <section class="section">
            <h2>âš™ï¸ æŠ€è¡“å®Ÿè£…</h2>

            <h3>QLoRA è¨­å®š</h3>
            <div class="code-block">
                <code>
from peft import LoraConfig, get_peft_model<br>
from transformers import BitsAndBytesConfig<br>
<br>
# 4-bité‡å­åŒ–è¨­å®š<br>
bnb_config = BitsAndBytesConfig(<br>
&nbsp;&nbsp;&nbsp;&nbsp;load_in_4bit=True,<br>
&nbsp;&nbsp;&nbsp;&nbsp;bnb_4bit_quant_type='nf4',  # Normal Float 4-bit<br>
&nbsp;&nbsp;&nbsp;&nbsp;bnb_4bit_compute_dtype=torch.bfloat16,<br>
&nbsp;&nbsp;&nbsp;&nbsp;bnb_4bit_use_double_quant=True,  # äºŒé‡é‡å­åŒ–<br>
)<br>
<br>
# LoRAè¨­å®š<br>
lora_config = LoraConfig(<br>
&nbsp;&nbsp;&nbsp;&nbsp;r=16,  # LoRA rank<br>
&nbsp;&nbsp;&nbsp;&nbsp;lora_alpha=32,<br>
&nbsp;&nbsp;&nbsp;&nbsp;target_modules=['q_proj', 'v_proj'],  # Attentionå±¤ã®ã¿<br>
&nbsp;&nbsp;&nbsp;&nbsp;lora_dropout=0.05,<br>
&nbsp;&nbsp;&nbsp;&nbsp;bias='none',<br>
)
                </code>
            </div>

            <h3>ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</h3>
            <ul>
                <li><strong>Gradient Accumulation:</strong> ãƒãƒƒãƒã‚µã‚¤ã‚º1 Ã— ç´¯ç©16ã‚¹ãƒ†ãƒƒãƒ— = å®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º16</li>
                <li><strong>Target Moduleså‰Šæ¸›:</strong> 7ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« â†’ 2ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆq_proj, v_projï¼‰ã«é™å®š</li>
                <li><strong>Max Sequence Lengthæœ€é©åŒ–:</strong> Twitteræ–‡å­—åˆ¶é™ã«åˆã‚ã›ã¦384ãƒˆãƒ¼ã‚¯ãƒ³ã«è¨­å®š</li>
                <li><strong>GPU Memoryå‰²ã‚Šå½“ã¦:</strong> max_memory={'0': '14GB', '1': '14GB'} ã§æ˜ç¤ºçš„ã«åˆ¶é™</li>
            </ul>

            <h3>å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</h3>
            <div class="code-block">
                <code>
training_args = TrainingArguments(<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_dir='./hiromtoon-llm',<br>
&nbsp;&nbsp;&nbsp;&nbsp;num_train_epochs=3,<br>
&nbsp;&nbsp;&nbsp;&nbsp;per_device_train_batch_size=1,<br>
&nbsp;&nbsp;&nbsp;&nbsp;gradient_accumulation_steps=16,<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate=2e-4,<br>
&nbsp;&nbsp;&nbsp;&nbsp;fp16=False,<br>
&nbsp;&nbsp;&nbsp;&nbsp;bf16=True,  # bfloat16 ã§é«˜é€ŸåŒ–<br>
&nbsp;&nbsp;&nbsp;&nbsp;logging_steps=10,<br>
&nbsp;&nbsp;&nbsp;&nbsp;eval_strategy='steps',<br>
&nbsp;&nbsp;&nbsp;&nbsp;eval_steps=100,<br>
&nbsp;&nbsp;&nbsp;&nbsp;save_steps=500,<br>
)
                </code>
            </div>
        </section>

        <!-- Training Results -->
        <section class="section">
            <h2>ğŸ“Š å­¦ç¿’çµæœ</h2>

            <h3>Losså€¤ã®æ¨ç§»</h3>
            <div class="highlight-box">
                <p>
                    <strong>åˆæœŸLoss:</strong> 7.83 (Epoch 0.0)<br>
                    <strong>1 Epochå®Œäº†æ™‚:</strong> 1.89 (Epoch 1.0)<br>
                    <strong>ç¾åœ¨ï¼ˆå­¦ç¿’ä¸­ï¼‰:</strong> ~1.89 (Epoch 1.1/3.0)
                </p>
                <p style="margin-top: 1rem;">
                    Losså€¤ãŒ7.83 â†’ 1.89 ã¸å¤§å¹…ã«æ”¹å–„ï¼ˆ76%å‰Šæ¸›ï¼‰ã€‚éå­¦ç¿’ã®å…†å€™ã‚‚ãªãã€é †èª¿ã«å­¦ç¿’ãŒé€²è¡Œã—ã¦ã„ã¾ã™ã€‚
                </p>
            </div>

            <h3>GPUä½¿ç”¨çŠ¶æ³</h3>
            <ul>
                <li><strong>GPU 0:</strong> 9.0GB / 16GB (56%)</li>
                <li><strong>GPU 1:</strong> 8.5GB / 16GB (53%)</li>
                <li><strong>åˆè¨ˆ:</strong> 17.5GB / 32GBï¼ˆæƒ³å®šå†…ã®ä½¿ç”¨é‡ï¼‰</li>
            </ul>

            <h3>å­¦ç¿’æ™‚é–“</h3>
            <p>
                æ¨å®šå®Œäº†æ™‚é–“: ç´„1.5ã€œ2æ—¥ï¼ˆ3 Epochå®Œäº†ã¾ã§ï¼‰<br>
                ç¾åœ¨é€²è¡Œä¸­: Epoch 1.1/3.0ï¼ˆé †èª¿ã«é€²è¡Œä¸­ï¼‰
            </p>
        </section>

        <!-- Technical Challenges -->
        <section class="section">
            <h2>ğŸ¯ æŠ€è¡“çš„ãƒãƒ£ãƒ¬ãƒ³ã‚¸</h2>

            <h3>1. ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ã¨ã®æˆ¦ã„</h3>
            <p>
                åˆæœŸè¨­å®šã§ã¯ CUDA Out of Memory ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã€‚Target Modulesã€Sequence Lengthã€Batch Sizeã‚’æ®µéšçš„ã«èª¿æ•´ã—ã€æœ€é©ãªãƒãƒ©ãƒ³ã‚¹ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚
            </p>

            <h3>2. ãƒ‡ãƒ¼ã‚¿å“è³ªã®å‘ä¸Š</h3>
            <p>
                ãƒªãƒ—ãƒ©ã‚¤ã‚„ãƒªãƒ„ã‚¤ãƒ¼ãƒˆã‚’å«ã‚ã‚‹ã¨æ–‡è„ˆãŒä¸å®Œå…¨ã«ãªã‚‹ãŸã‚ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ„ã‚¤ãƒ¼ãƒˆã®ã¿ã«çµã‚Šè¾¼ã¿ã€‚ã•ã‚‰ã«ã€Gemma3ã® instruction format ã«æ­£ç¢ºã«å¤‰æ›ã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã¾ã—ãŸã€‚
            </p>

            <h3>3. ãƒ¢ãƒ‡ãƒ«é¸å®š</h3>
            <p>
                å½“åˆ Gemma2-27B ã‚’æ¤œè¨ã—ã¦ã„ã¾ã—ãŸãŒã€æœ€æ–°ã® Gemma3-27B ã«å¤‰æ›´ã€‚ã‚ˆã‚Šé«˜ã„æ€§èƒ½ã¨ã€HuggingFace ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯å–å¾—ã«ã‚ˆã‚Šã€æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã§ã®å­¦ç¿’ã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚
            </p>
        </section>

        <!-- Future Work -->
        <section class="section">
            <h2>ğŸš€ ä»Šå¾Œã®å±•é–‹</h2>
            <ul>
                <li><strong>OpenWebUIçµ±åˆ:</strong> OllamaçµŒç”±ã§æ—¥å¸¸çš„ã«åˆ©ç”¨å¯èƒ½ã«</li>
                <li><strong>ç¶™ç¶šå­¦ç¿’:</strong> æ–°ã—ã„ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã§å®šæœŸçš„ã«è¿½åŠ å­¦ç¿’</li>
                <li><strong>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ:</strong> ç”»åƒä»˜ããƒ„ã‚¤ãƒ¼ãƒˆã‚‚å­¦ç¿’å¯¾è±¡ã«</li>
                <li><strong>ä¼šè©±ãƒ­ã‚°çµ±åˆ:</strong> Twitterä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚‚å­¦ç¿’ã«è¿½åŠ </li>
                <li><strong>æ€§èƒ½è©•ä¾¡:</strong> äººé–“è©•ä¾¡ã«ã‚ˆã‚‹å“è³ªæ¸¬å®šã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯</li>
            </ul>
        </section>

        <!-- Conclusion -->
        <section class="section">
            <h2>ğŸ’¡ å­¦ã‚“ã ã“ã¨</h2>
            <ul>
                <li>QLoRAã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªLLMãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•</li>
                <li>å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå®…GPUç’°å¢ƒã§å­¦ç¿’ã™ã‚‹ãŸã‚ã®æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</li>
                <li>PyTorchã€HuggingFace Transformersã€PEFT ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å®Ÿè·µçš„ãªä½¿ç”¨æ–¹æ³•</li>
                <li>CUDA ãƒ¡ãƒ¢ãƒªç®¡ç†ã¨ Multi-GPU ç’°å¢ƒã§ã®ãƒ¢ãƒ‡ãƒ«åˆ†æ•£é…ç½®</li>
                <li>ãƒ‡ãƒ¼ã‚¿å“è³ªãŒLLMå­¦ç¿’ã«ä¸ãˆã‚‹å½±éŸ¿ã®é‡è¦æ€§</li>
            </ul>
        </section>

        <!-- Links -->
        <section class="section">
            <h2>ğŸ”— é–¢é€£ãƒªãƒ³ã‚¯</h2>
            <div class="hero-buttons">
                <a href="https://github.com/hiromtoon/hiromtoon-llm" class="btn btn-primary" target="_blank">
                    GitHub Repository
                </a>
            </div>
        </section>

        <a href="../index.html#projects" class="back-link">â† Back to Projects</a>
    </main>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>&copy; 2025 Hiromtoon. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
